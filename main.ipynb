{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560804 entries, 0 to 560803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    560804 non-null int64\n",
      "HelpfulnessNumerator      560804 non-null int64\n",
      "Id                        560804 non-null int64\n",
      "ProductId                 560804 non-null object\n",
      "Score                     460804 non-null float64\n",
      "Summary                   560778 non-null object\n",
      "Text                      560804 non-null object\n",
      "Time                      560804 non-null int64\n",
      "UserId                    560804 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 38.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time</th>\n",
       "      <th>UserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130058</td>\n",
       "      <td>B000CQIDHY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A worthy and welcome replacement</td>\n",
       "      <td>I don't know what has happened to formulation ...</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>A3VZR9TPF2GERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91622</td>\n",
       "      <td>B004YV80OE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It was okay, good flavor</td>\n",
       "      <td>Kraft's a safe brand. They will produce food f...</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>A1B1QMGK8VYG80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>699</td>\n",
       "      <td>B000G6MBX2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The \"Organic\" Label is Misleading</td>\n",
       "      <td>\"Yeast Extract\" is listed as an ingredient. So...</td>\n",
       "      <td>1195084800</td>\n",
       "      <td>A1AQ2W2R4SOVGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265935</td>\n",
       "      <td>B0001GDC4O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fresh/Stale</td>\n",
       "      <td>Some of these espresso pods were fresh and som...</td>\n",
       "      <td>1272499200</td>\n",
       "      <td>A2IVH1D3GLACL3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199932</td>\n",
       "      <td>B000EDG430</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Baked to perfection in my bread machine!</td>\n",
       "      <td>I am not one to write reviews, but this bread ...</td>\n",
       "      <td>1336953600</td>\n",
       "      <td>AEOINN8F4D9DQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HelpfulnessDenominator  HelpfulnessNumerator      Id   ProductId  Score  \\\n",
       "0                       0                     0  130058  B000CQIDHY    5.0   \n",
       "1                       0                     0   91622  B004YV80OE    4.0   \n",
       "2                      10                     6     699  B000G6MBX2    1.0   \n",
       "3                       0                     0  265935  B0001GDC4O    5.0   \n",
       "4                       1                     1  199932  B000EDG430    5.0   \n",
       "\n",
       "                                    Summary  \\\n",
       "0          A worthy and welcome replacement   \n",
       "1                  It was okay, good flavor   \n",
       "2         The \"Organic\" Label is Misleading   \n",
       "3                               Fresh/Stale   \n",
       "4  Baked to perfection in my bread machine!   \n",
       "\n",
       "                                                Text        Time  \\\n",
       "0  I don't know what has happened to formulation ...  1337817600   \n",
       "1  Kraft's a safe brand. They will produce food f...  1317254400   \n",
       "2  \"Yeast Extract\" is listed as an ingredient. So...  1195084800   \n",
       "3  Some of these espresso pods were fresh and som...  1272499200   \n",
       "4  I am not one to write reviews, but this bread ...  1336953600   \n",
       "\n",
       "           UserId  \n",
       "0  A3VZR9TPF2GERB  \n",
       "1  A1B1QMGK8VYG80  \n",
       "2  A1AQ2W2R4SOVGN  \n",
       "3  A2IVH1D3GLACL3  \n",
       "4   AEOINN8F4D9DQ  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 460804 entries, 0 to 460803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    460804 non-null int64\n",
      "HelpfulnessNumerator      460804 non-null int64\n",
      "Id                        460804 non-null int64\n",
      "ProductId                 460804 non-null object\n",
      "Score                     460804 non-null float64\n",
      "Summary                   460782 non-null object\n",
      "Text                      460804 non-null object\n",
      "Time                      460804 non-null int64\n",
      "UserId                    460804 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 35.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 460804 to 560803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    100000 non-null int64\n",
      "HelpfulnessNumerator      100000 non-null int64\n",
      "Id                        100000 non-null int64\n",
      "ProductId                 100000 non-null object\n",
      "Score                     0 non-null float64\n",
      "Summary                   99996 non-null object\n",
      "Text                      100000 non-null object\n",
      "Time                      100000 non-null int64\n",
      "UserId                    100000 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Score']==df['Score']]\n",
    "test = df[df['Score']!=df['Score']]\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         for word in word_tokenize(sent))\n\u001b[0;32m----> 8\u001b[0;31m         for message in train.Text]\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         for word in word_tokenize(sent))\n\u001b[0;32m----> 8\u001b[0;31m         for message in train.Text]\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m stemmed_train_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         for word in word_tokenize(sent))\n\u001b[0m\u001b[1;32m      8\u001b[0m         for message in train.Text]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The language '{0}' is not supported.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mstemmerclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Stemmer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmerclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ignore_stopwords)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0;32m---> 23\u001b[0;31m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# stemmed_train_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "#          for sent in sent_tokenize(message)\n",
    "#         for word in word_tokenize(sent))\n",
    "#         for message in train.Text]\n",
    "\n",
    "# stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "#          for sent in sent_tokenize(message)\n",
    "#         for word in word_tokenize(sent))\n",
    "#         for message in test.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec 1 finish\n",
      "vec 2 finish\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2, ngram_range=(1,2))\n",
    "text_vec_tr = vectorizer.fit_transform(train.Text)\n",
    "text_vec_test = vectorizer.transform(test.Text)\n",
    "print('vec 1 finish')\n",
    "vectorizer_sum = TfidfVectorizer(stop_words='english', min_df=2, ngram_range=(1,2))\n",
    "sum_vec_tr = vectorizer.fit_transform(train.Summary.values.astype('U'))\n",
    "sum_vec_test = vectorizer.transform(test.Summary.values.astype('U'))\n",
    "print('vec 2 finish')\n",
    "y = train.Score.values\n",
    "X_train = hstack([sum_vec_tr,text_vec_tr])\n",
    "X_test = hstack([sum_vec_test,text_vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: 0.1\n",
      "reg: 0.4\n",
      "reg: 0.7\n",
      "reg: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'min_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cc78868a26ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mbest_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmin_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'min_score' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "best_C = 0.1\n",
    "max_score = 0\n",
    "for reg in np.arange(0.1, 1.0, 0.3):\n",
    "    print('reg:',reg)\n",
    "    clf = LogisticRegression(C=reg, solver='saga')\n",
    "    scores = cross_val_score(clf, X_train, y, cv=3)\n",
    "    s = scores.mean()\n",
    "    if (s > max_score):\n",
    "        best_C = reg\n",
    "        max_score = s\n",
    "        \n",
    "\n",
    "for reg in np.arange(1, 21, 2):\n",
    "    print('reg:',reg)\n",
    "    clf = LogisticRegression(C=reg, solver='saga')\n",
    "    scores = cross_val_score(clf, X_train, y, cv=3)\n",
    "    s = scores.mean()\n",
    "    if (s > min_score):\n",
    "        best_C = reg\n",
    "        min_score = s\n",
    "\n",
    "        \n",
    "print('the best C:', best_C)\n",
    "print('max score:',max_score)\n",
    "# def lr_predict(X_train, X_test, y_train, y_test, reg):\n",
    "#     # Logistic regression\n",
    "#     lr = LogisticRegression(C=reg, solver='saga')\n",
    "#     lr.fit(X_train, y)\n",
    "#     y_pred_lr = lr.predict(X_test)\n",
    "#     rms = sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "#     return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "res = pd.DataFrame(data={'Id':test.Id, 'Score':y_pred_lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save CSV\n",
    "res.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5., ...,  5.,  5.,  5.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "y_pred_nb = nb.fit(text_vec_tr, train.Score.values).predict(text_vec_test)\n",
    "# rms = sqrt(mean_squared_error(test.Score, y_pred_nb))\n",
    "res = pd.DataFrame(data={'Id':test.Id, 'Score':y_pred_nb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allegro_l/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:532: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2861\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2862\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2863\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d88ec805e65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vec_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# initialize parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# change to perplexity later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_init_latent_vars\u001b[0;34m(self, n_features)\u001b[0m\n\u001b[1;32m    338\u001b[0m         self.exp_dirichlet_component_ = np.exp(\n\u001b[0;32m--> 339\u001b[0;31m             _dirichlet_expectation_2d(self.components_))\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# lda = LatentDirichletAllocation(n_components=2000)\n",
    "# lda.fit(text_vec_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd_sum = TruncatedSVD(n_components=50)\n",
    "dm_text_vec_tr = svd.fit_transform(text_vec_tr)\n",
    "dm_sum_vec_tr = svd_sum.fit_transform(sum_vec_tr)\n",
    "\n",
    "# concatenate feature vec after dimension reduction\n",
    "dm_sum_vec_test = svd_sum.transform(sum_vec_test)\n",
    "dm_text_vec_test = svd.transform(text_vec_test)\n",
    "from scipy.sparse import hstack\n",
    "X_train = np.concatenate((dm_sum_vec_tr,dm_text_vec_tr), axis=1)\n",
    "X_test = np.concatenate((dm_sum_vec_test,dm_text_vec_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
