{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 506 HW4 code\n",
    "The only required data file is `train.csv` from Kaggle class website. This CSV file should be put in the current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560804 entries, 0 to 560803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    560804 non-null int64\n",
      "HelpfulnessNumerator      560804 non-null int64\n",
      "Id                        560804 non-null int64\n",
      "ProductId                 560804 non-null object\n",
      "Score                     460804 non-null float64\n",
      "Summary                   560777 non-null object\n",
      "Text                      560804 non-null object\n",
      "Time                      560804 non-null int64\n",
      "UserId                    560804 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 38.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Time</th>\n",
       "      <th>UserId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130058</td>\n",
       "      <td>B000CQIDHY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A worthy and welcome replacement</td>\n",
       "      <td>I don't know what has happened to formulation ...</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>A3VZR9TPF2GERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91622</td>\n",
       "      <td>B004YV80OE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It was okay, good flavor</td>\n",
       "      <td>Kraft's a safe brand. They will produce food f...</td>\n",
       "      <td>1317254400</td>\n",
       "      <td>A1B1QMGK8VYG80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>699</td>\n",
       "      <td>B000G6MBX2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The \"Organic\" Label is Misleading</td>\n",
       "      <td>\"Yeast Extract\" is listed as an ingredient. So...</td>\n",
       "      <td>1195084800</td>\n",
       "      <td>A1AQ2W2R4SOVGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265935</td>\n",
       "      <td>B0001GDC4O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fresh/Stale</td>\n",
       "      <td>Some of these espresso pods were fresh and som...</td>\n",
       "      <td>1272499200</td>\n",
       "      <td>A2IVH1D3GLACL3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199932</td>\n",
       "      <td>B000EDG430</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Baked to perfection in my bread machine!</td>\n",
       "      <td>I am not one to write reviews, but this bread ...</td>\n",
       "      <td>1336953600</td>\n",
       "      <td>AEOINN8F4D9DQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HelpfulnessDenominator  HelpfulnessNumerator      Id   ProductId  Score  \\\n",
       "0                       0                     0  130058  B000CQIDHY    5.0   \n",
       "1                       0                     0   91622  B004YV80OE    4.0   \n",
       "2                      10                     6     699  B000G6MBX2    1.0   \n",
       "3                       0                     0  265935  B0001GDC4O    5.0   \n",
       "4                       1                     1  199932  B000EDG430    5.0   \n",
       "\n",
       "                                    Summary  \\\n",
       "0          A worthy and welcome replacement   \n",
       "1                  It was okay, good flavor   \n",
       "2         The \"Organic\" Label is Misleading   \n",
       "3                               Fresh/Stale   \n",
       "4  Baked to perfection in my bread machine!   \n",
       "\n",
       "                                                Text        Time  \\\n",
       "0  I don't know what has happened to formulation ...  1337817600   \n",
       "1  Kraft's a safe brand. They will produce food f...  1317254400   \n",
       "2  \"Yeast Extract\" is listed as an ingredient. So...  1195084800   \n",
       "3  Some of these espresso pods were fresh and som...  1272499200   \n",
       "4  I am not one to write reviews, but this bread ...  1336953600   \n",
       "\n",
       "           UserId  \n",
       "0  A3VZR9TPF2GERB  \n",
       "1  A1B1QMGK8VYG80  \n",
       "2  A1AQ2W2R4SOVGN  \n",
       "3  A2IVH1D3GLACL3  \n",
       "4   AEOINN8F4D9DQ  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe into training set and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 460804 entries, 0 to 460803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    460804 non-null int64\n",
      "HelpfulnessNumerator      460804 non-null int64\n",
      "Id                        460804 non-null int64\n",
      "ProductId                 460804 non-null object\n",
      "Score                     460804 non-null float64\n",
      "Summary                   460782 non-null object\n",
      "Text                      460804 non-null object\n",
      "Time                      460804 non-null int64\n",
      "UserId                    460804 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 35.2+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 460804 to 560803\n",
      "Data columns (total 9 columns):\n",
      "HelpfulnessDenominator    100000 non-null int64\n",
      "HelpfulnessNumerator      100000 non-null int64\n",
      "Id                        100000 non-null int64\n",
      "ProductId                 100000 non-null object\n",
      "Score                     0 non-null float64\n",
      "Summary                   99996 non-null object\n",
      "Text                      100000 non-null object\n",
      "Time                      100000 non-null int64\n",
      "UserId                    100000 non-null object\n",
      "dtypes: float64(1), int64(4), object(4)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train = df[df['Score']==df['Score']]\n",
    "test = df[df['Score']!=df['Score']]\n",
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate tf-idf for both **text review** and **summary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec 1 finish\n",
      "vec 2 finish\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2, ngram_range=(1,2))\n",
    "text_vec_tr = vectorizer.fit_transform(train.Text)\n",
    "text_vec_test = vectorizer.transform(test.Text)\n",
    "print('vec 1 finish')\n",
    "vectorizer_sum = TfidfVectorizer(stop_words='english', min_df=2, ngram_range=(1,2))\n",
    "sum_vec_tr = vectorizer.fit_transform(train.Summary.values.astype('U'))\n",
    "sum_vec_test = vectorizer.transform(test.Summary.values.astype('U'))\n",
    "print('vec 2 finish')\n",
    "y = train.Score.values\n",
    "X_train = hstack([sum_vec_tr,text_vec_tr])\n",
    "X_test = hstack([sum_vec_test,text_vec_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation (3 fold) for logistic regression regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg: 0.1\n",
      "reg: 0.4\n",
      "reg: 0.7\n",
      "reg: 1\n",
      "reg: 3\n",
      "reg: 5\n",
      "reg: 7\n",
      "reg: 9\n",
      "reg: 11\n",
      "reg: 13\n",
      "reg: 15\n",
      "reg: 17\n",
      "reg: 19\n",
      "the best C: 9\n",
      "max score: 0.827037080628\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_C = 0.1\n",
    "max_score = 0\n",
    "for reg in np.arange(0.1, 1.0, 0.3):\n",
    "    print('reg:',reg)\n",
    "    clf = LogisticRegression(C=reg, solver='saga')\n",
    "    scores = cross_val_score(clf, X_train, y, cv=3)\n",
    "    s = scores.mean()\n",
    "    if (s > max_score):\n",
    "        best_C = reg\n",
    "        max_score = s\n",
    "        \n",
    "\n",
    "for reg in np.arange(1, 21, 2):\n",
    "    print('reg:',reg)\n",
    "    clf = LogisticRegression(C=reg, solver='saga')\n",
    "    scores = cross_val_score(clf, X_train, y, cv=3)\n",
    "    s = scores.mean()\n",
    "    if (s > max_score):\n",
    "        best_C = reg\n",
    "        max_score = s\n",
    "\n",
    "        \n",
    "print('the best C:', best_C)\n",
    "print('max score:',max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the result for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=best_C, solver='saga')\n",
    "lr.fit(X_train, y)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "res = pd.DataFrame(data={'Id':test.Id, 'Score':y_pred_lr})\n",
    "res.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Naive Bayes for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "y_pred_nb = nb.fit(X_train, y).predict(X_test)\n",
    "res = pd.DataFrame(data={'Id':test.Id, 'Score':y_pred_nb})\n",
    "res.to_csv('result_nb.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply SVD to the tf-idf vectors:\n",
    "\n",
    "Here I reduce the feature of summary to 50 dim and feature of text reviews to 500 dim.\n",
    "The output X_train and X_test are applied again in the previous cell conducting logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "svd_sum = TruncatedSVD(n_components=50)\n",
    "dm_text_vec_tr = svd.fit_transform(text_vec_tr)\n",
    "dm_sum_vec_tr = svd_sum.fit_transform(sum_vec_tr)\n",
    "\n",
    "# concatenate feature vec after dimension reduction\n",
    "dm_sum_vec_test = svd_sum.transform(sum_vec_test)\n",
    "dm_text_vec_test = svd.transform(text_vec_test)\n",
    "from scipy.sparse import hstack\n",
    "X_train = np.concatenate((dm_sum_vec_tr,dm_text_vec_tr), axis=1)\n",
    "X_test = np.concatenate((dm_sum_vec_test,dm_text_vec_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming could be applied, but it took too long time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         for word in word_tokenize(sent))\n\u001b[0;32m----> 8\u001b[0;31m         for message in train.Text]\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         for word in word_tokenize(sent))\n\u001b[0;32m----> 8\u001b[0;31m         for message in train.Text]\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
      "\u001b[0;32m<ipython-input-61-2c6ea4984e47>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m stemmed_train_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n\u001b[1;32m      6\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         for word in word_tokenize(sent))\n\u001b[0m\u001b[1;32m      8\u001b[0m         for message in train.Text]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The language '{0}' is not supported.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mstemmerclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Stemmer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmerclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ignore_stopwords)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0;32m---> 23\u001b[0;31m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "stemmed_train_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in train.Text]\n",
    "\n",
    "stemmed_test_text = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in test.Text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
